# Research Agent Lessons

This file contains lessons learned from past research sessions. The research agent should consult these before cutting cards.

## Card Quality

### Tags Must Be Specific Claims
- BAD: "Economic impacts of TikTok"
- GOOD: "TikTok ban eliminates 100,000 creator jobs"
- The tag should state exactly what the card PROVES, not just what it's about

### Purpose Must Explain Strategic Use
- BAD: "Shows economic harm"
- GOOD: "Use to outweigh opponent's security benefits - concrete job loss number beats vague threats"
- Explain WHEN and HOW to use the card in a round

### Bold the Right Amount
- Target 20-40% of text bolded
- Bold the KEY WARRANTS - the claims that do the argumentative work
- Don't bold context, attribution, or setup sentences

## Organization

### Section Headers Must Be Specific
- BAD: "Answer to economic impacts"
- GOOD: "Answer to: Opponent claim that ban protects American jobs"
- If the card answers a specific argument, name that argument
- If the card is general (applies to many arguments), say so

### Support vs Answer vs Extension vs Impact
- **Support**: Proves YOUR claim directly
- **Answer**: Responds to THEIR specific argument
- **Extension**: Adds warrants to an existing argument
- **Impact**: Shows magnitude/timeframe/probability of harm or benefit

## Side Awareness

### PRO vs CON Relative to Resolution
- PRO = affirms/supports the resolution
- CON = negates/opposes the resolution
- For "Resolved: The US should ban TikTok":
  - PRO supports the ban
  - CON opposes the ban
- Double-check that cards actually support the assigned side!

## Source Quality

### Prioritize Credible Sources
- Academic journals and university research
- Major news outlets with fact-checking
- Think tanks and policy institutes
- Government reports and official data
- Expert commentary (check credentials!)

### Recency Matters
- Prioritize 2023-2026 evidence
- Older evidence okay for historical context or foundational concepts
- Be wary of outdated statistics

## Query Strategy

### Multi-Strategy Queries Find More
- Single naive queries often miss good sources
- Combine multiple query types:
  - **Exploratory**: Broad discovery queries
  - **Spearfish**: Specific claim + year targeting
  - **Source-targeted**: Search within known credible sources (brookings.edu, cfr.org)
  - **Expert**: Find professors, analysts, researchers
  - **Verbatim**: Exact phrase matching for related sources

### Avoid Duplication
- Check existing coverage before researching
- If you have 3+ cards on a claim, low value to add more of same type
- Higher value: Different evidence TYPE on same claim (statistical vs analytical)
- Follow citations in existing cards to find related sources

## Evidence Types

### Classify Every Card
- **statistical**: Numbers, percentages, dollar amounts, job counts
- **analytical**: Expert reasoning, causal explanations
- **consensus**: Multiple sources agreeing, institutional positions
- **empirical**: Case studies, real-world examples
- **predictive**: Forecasts, projections, "will likely"

### Seek Type Diversity
- One strong statistical card is good
- Statistical + analytical is better
- Statistical + analytical + consensus is best
- Different types reinforce each other

## Explore vs Exploit

### When to Explore
- Early in prep (discover argument landscape)
- After diminishing returns (research yields 0 cards)
- When opponent coverage is low
- When stuck on one topic

### When to Exploit
- After identifying key arguments
- When claims have thin coverage (1-2 cards)
- Building comprehensive blocks
- Late in prep with limited budget

---

*Add new lessons below as they're learned:*
